{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "r", 
            "name": "R", 
            "file_extension": ".r", 
            "mimetype": "text/x-r-source", 
            "codemirror_mode": "r", 
            "version": "3.3.2"
        }, 
        "kernelspec": {
            "name": "r", 
            "language": "R", 
            "display_name": "R with Spark 1.6"
        }
    }, 
    "cells": [
        {
            "source": "# Use Spark for R to load data and run SQL queries\nThis notebook introduces basic Spark concepts and helps you to start using Spark for R.\n\nSome familiarity with R is recommended.\n\nIn this notebook, you'll use the publicly available **mtcars** data set from *Motor Trend* magazine to learn some basic R. You'll learn how to load data, create a Spark DataFrame, aggregate data, run mathematical formulas, and run SQL queries against the data.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Table of contents\nThis notebook contains these main sections:\n\n1. [Load a DataFrame](#Load_a_DataFrame)\n2. [Initialize an SQLContext](#Initialize_an_SQLContext)\n3. [Create a Spark DataFrame](#Create_a_Spark_DataFrame)\n4. [Aggregate data after grouping by columns](#Aggregate_data_after_grouping_by_columns)\n5. [Operate on columns](#Operate_on_columns)\n6. [Run SQL queries from the Spark DataFrame](#Run_SQL_queries_from_the_Spark_DataFrame)", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "<a id='Load_a_DataFrame'></a>\n## 1. Load a DataFrame\nA DataFrame is a distributed collection of data that is organized into named columns. The built-in R DataFrame called **mtcars** includes observations on the following 11 variables:\n\n`[, 1]\tmpg     Miles / (US) gallon`  \n`[, 2]\tcyl     Number of cylinders`  \n`[, 3]\tdisp\tDisplacement (cu. in.)`  \n`[, 4]\thp      Gross horsepower`  \n`[, 5]\tdrat    Rear axle ratio`  \n`[, 6]\twt      Weight (1000 lbs)`  \n`[, 7]\tqsec    1/4 mile time (seconds)`  \n`[, 8]\tvs      0 = V-engine, 1 = straight engine`  \n`[, 9]\tam      Transmission (0 = automatic, 1 = manual)`  \n`[,10]\tgear    Number of forward gears`  \n`[,11]\tcarb    Number of carburetors`", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "Preview the first 3 rows of the DataFrame by using the head() function:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "head(mtcars, 3)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "Convert the car name data, which appears in the row names, into an actual column so that Spark can read it as a column:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "mtcars$car <- rownames(mtcars)\nmtcars <- mtcars[,c(12,1:11)]\nrownames(mtcars) <- 1:nrow(mtcars)\nhead(mtcars)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "<a id='Initialize_an_SQLContext'></a>\n## 2. Initialize an SQLContext\nTo work with a DataFrame, you need an SQLContext. You create this SQLContext by using `sparkRSQL.init(sc)`. A SparkContext named sc, which has been created for you, is used to initialize the SQLContext:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "sqlContext <- sparkRSQL.init(sc)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "<a id='Create_a_Spark_DataFrame'></a>\n## 3. Create a Spark DataFrame\nUsing the SQLContext and the loaded local DataFrame, create a Spark DataFrame and print the schema, or structure, of the DataFrame:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "sdf <- createDataFrame(sqlContext, mtcars) \nprintSchema(sdf)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "Display the content of the Spark DataFrame:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "SparkR::head(sdf, 32)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true, 
                "collapsed": false
            }
        }, 
        {
            "source": "Try different ways of retrieving subsets of the data. For example, get the first 5 values in the **mpg** column:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "SparkR::head(select(sdf, sdf$mpg),5)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true, 
                "collapsed": false
            }
        }, 
        {
            "source": "Filter the DataFrame to retain only rows with **mpg** values that are less than 18:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "SparkR::head(SparkR::filter(sdf, sdf$mpg < 18))", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "<a id='Aggregate_data_after_grouping_by_columns'></a>\n## 4. Aggregate data after grouping by columns\nSpark DataFrames support a number of common functions to aggregate data after grouping. For example, you can compute the average weight of cars as a function of the number of cylinders:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "SparkR::head(summarize(groupBy(sdf, sdf$cyl), wtavg = avg(sdf$wt)))", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "You can also sort the output from the aggregation to determine the most popular cylinder configuration in the DataFrame:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "car_counts <-summarize(groupBy(sdf, sdf$cyl), count = n(sdf$wt))\nSparkR::head(arrange(car_counts, desc(car_counts$count)))", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "<a id='Operate_on_columns'></a>\n## 5. Operate on columns\nSparkR provides a number of functions that you can apply directly to columns for data processing. In the following example, a basic arithmetic function converts lbs to metric tons:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "sdf$wtTon <- sdf$wt * 0.45\nSparkR::head(select(sdf, sdf$car, sdf$wt, sdf$wtTon),6)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "<a id='Run_SQL_queries_from_the_Spark_DataFrame'></a>\n## 6. Run SQL queries from the Spark DataFrame\nYou can register a Spark DataFrame as a temporary table and then run SQL queries over the data. The `sql` function enables an application to run SQL queries programmatically and returns the result as a DataFrame:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "registerTempTable(sdf, \"cars\")\n\nhighgearcars <- sql(sqlContext, \"SELECT car, gear FROM cars WHERE gear >= 5\")\nSparkR::head(highgearcars)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## That's it!\nYou successfully completed this notebook! You learned how to load a DataFrame, view and filter the data, aggregate the data, perform operations on the data in specific columns, and run SQL queries against the data. For more information about Spark, see the [Spark Quick Start Guide](http://spark.apache.org/docs/latest/quick-start.html).", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Want to learn more?\n### Free courses on <a href=\"https://bigdatauniversity.com/courses/?utm_source=tutorial-dashdb-python&utm_medium=github&utm_campaign=bdu/\" rel=\"noopener noreferrer\" target=\"_blank\">Big Data University</a>: <a href=\"https://bigdatauniversity.com/courses/?utm_source=tutorial-dashdb-python&utm_medium=github&utm_campaign=bdu\" rel=\"noopener noreferrer\" target=\"_blank\"><img src = \"https://ibm.box.com/shared/static/xomeu7dacwufkoawbg3owc8wzuezltn6.png\" width=600px> </a>", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### Authors\n\n**Saeed Aghabozorgi**, PhD, is a Data Scientist in IBM with a track record of developing enterprise-level applications that substantially increases clients' ability to turn data into actionable knowledge. He is a researcher in the data mining field and an expert in developing advanced analytic methods like machine learning and statistical modelling on large data sets.\n\n**Polong Lin** is a Data Scientist at IBM in Canada. Under the Emerging Technologies division, Polong is responsible for educating the next generation of data scientists through Big Data University. Polong is a regular speaker in conferences and meetups, and holds an M.Sc. in Cognitive Psychology.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "Copyright \u00a9 2016 Big Data University. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\" rel=\"noopener noreferrer\" target=\"_blank\">MIT License</a>.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }
    ], 
    "nbformat_minor": 0
}